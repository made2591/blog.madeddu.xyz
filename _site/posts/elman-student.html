<!DOCTYPE html>
<html>
  <head>
    <!-- Google Tag Manager -->
    <!--<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-M8WBXH7');</script>-->
    <!-- End Google Tag Manager -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,300,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href=/public/stylesheets/bs.css?random=@Environment.TickCount>
    <link rel="stylesheet" href=/public/stylesheets/styles.css?random=@Environment.TickCount>
    <link rel="stylesheet" href=/public/stylesheets/pygment_trac.css?random=@Environment.TickCount>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.1/css/font-awesome.min.css">
    <link rel="canonical" href="made2591.github.io/posts/elman-student">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
      <link rel="icon" type="image/x-icon"  href="/favicon.ico" />
    <title>How my Elman network learnt to count | Matteo Madeddu</title>
    <!--[if lt IE 9]>

      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
  </head>
  <body>
    <!-- Google Tag Manager (noscript) -->
    <!--<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-M8WBXH7"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->
    <div class="wrapper">
        <section>
          <div id="header">
  <h1>
    <a id="sitename" href="">Matteo Madeddu</a>
  </h1>

  <!-- Add something about you in p tag-->
  <p>Mac OS lover, Docker fan, Go explorer, Python geek, Trello addicted.</p>
  <hr/>

  <span class="credits pull-left">
    
      <a href="/">Home</a>  | 
    
      <a href="/blog">Blog</a>  | 
    
      <a href="/archive">Archive</a>  | 
    
      <a href="/about">About</a>  | 
    
      <a href="/matteo_madeddu_cv.pdf">Resume</a>  | 
    
      <a href="/quotes">Quotes</a> 
    
  </span>

  <span class="credits pull-right social">
    
      
        <a href="https://github.com/made2591/" target="_blank"><i class="fa fa-github"></i></a>
      
    
      
        <a href="https://twitter.com/made2591" target="_blank"><i class="fa fa-twitter"></i></a>
      
    
      
        <a href="https://linkedin.com/in/mmadeddu" target="_blank"><i class="fa fa-linkedin"></i></a>
      
    
      
        <a href="https://www.facebook.com/matteo.madeddu" target="_blank"><i class="fa fa-facebook"></i></a>
      
    
  </span>


</div>


          <div class="post-title">
    <h1>How my Elman network learnt to count</h1>
    <p class="text-muted">
    


    13 Nov 2018
    
     | <i class="fa fa-comment"></i> <a class="text-muted" href="/posts/elman-student/#disqus_thread"></a>
    

    
      | <i class="fa fa-tag"></i>
      
        <a class="text-muted" href="/tags/#coding ">coding</a>,
      
        <a class="text-muted" href="/tags/#golang ">golang</a>,
      
        <a class="text-muted" href="/tags/#ann ">ann</a>,
      
        <a class="text-muted" href="/tags/#elman ">elman</a>,
      
        <a class="text-muted" href="/tags/#adding ">adding</a>,
      
        <a class="text-muted" href="/tags/#neural ">neural</a>,
      
        <a class="text-muted" href="/tags/#networks ">networks</a>
      
    
  </p>
</div>

  <h3 id="introduction">Introduction</h3>
<p>This is actually a sort of back-to-the-future post because it’s related to something I completed one year ago: I built this Elman network and it learnt to count. What I shame, I forgot it, now it’s kind of its first birthday so let’s celebrate :D</p>

<p align="center"><img src="https://static1.squarespace.com/static/550ca181e4b00ab6c2a10330/t/55afd73be4b0ba2638779743/1437587260614/boy-going-back-to-school.jpg?format=750w" alt="matrixbug" style="width: 100%; marker-top: -10px;" /></p>

<p>This is Elman, the best in class in adding int32 numbers. For everybody who already knows what I will talk about (what?!), <a href="https://github.com/made2591/go-perceptron-go">here</a>’s the Github repo. I’m sorry for the name, it’s still go-perceptron-go but that repo contains my GoLang ANN.</p>

<h3 id="lets-start-from">Let’s start from</h3>
<p>You were wondering what the f**k is an Elman network: to be honest, I didn’t understand exactly but <a href="https://made2591.github.io/posts/neuralnetwork">this</a> post related to the perceptron could be a good starting point - at least, somehow linked cause in the end this network share a lot with multilayer perceptron. Ignored? Perfect. In one sentence: an Elmann network is a MFNN with an extra context layer. That is a Multilayer Feedforward Neural Network with an extra context layer: the point is that, unfortunately, this context layer create a closed circle in the network - thus, in the way the information is progated.</p>

<p>That means that Elman network are actually RNN, or Recurrent Neural Network even. That are…wait. Let’s make a step back.</p>

<p align="center"><img src="https://i.imgur.com/GbUJP5R.png" alt="matrixbug" style="width: 40%; marker-top: -10px;" /></p>

<h4 id="rnns-vs-standard-anns">RNNs vs Standard ANNs</h4>
<p>As you know an ANN can be described as a set of neuron units (read perceptron), organized in layers, linked together in several ways to achieve specific - mainly classification - jobs. What it came out is that by changing the links used to attach the neural network layers you can expect different behaviour. What does it mean changing the way the information flow?</p>

<p>The idea behind RNNs is to make use of <em>sequential information</em>. In a traditional neural network we assume that all inputs (and outputs) are independent between each other but, for many tasks… that’s a very bad idea. For instance, if you want to predict the next word in a sentence you better know which words came before it.</p>

<p>RNNs are called <em>recurrent</em> because they perform the same task for every element of a sequence, with the output being depended on the previous computations. Another way to think about RNNs is that they have a <em>memory</em> which captures information about what has been calculated so far. In theory RNNs can make use of information in arbitrarily long sequences, but in practice they are limited to looking back only a few steps.</p>

<h4 id="base-structures---code">Base structures - <a href="https://github.com/made2591/go-perceptron-go/tree/master/model/neural">code</a></h4>
<p>To create a neural network, the first thing you have to do is dealing with the definition of data structures. I create a <code class="highlighter-rouge">neural</code> package to collect all files related to architecture structure and elements.</p>

<h5 id="pattern---code">Pattern - <a href="https://github.com/made2591/go-perceptron-go/blob/master/model/neural/pattern.go">code</a></h5>
<p>The <code class="highlighter-rouge">Pattern</code> struct represent a single input struct. Look at the code:</p>

<figure class="highlight"><pre><code class="language-golang" data-lang="golang"><span class="c">// Pattern struct represents one pattern with dimensions and desired value</span><span class="x">
</span><span class="k">type</span><span class="x"> </span><span class="n">Pattern</span><span class="x"> </span><span class="k">struct</span><span class="x"> </span><span class="p">{</span><span class="x">
	</span><span class="n">Features</span><span class="x"> </span><span class="p">[]</span><span class="kt">float64</span><span class="x">
	</span><span class="n">SingleRawExpectation</span><span class="x"> </span><span class="kt">string</span><span class="x">
	</span><span class="n">SingleExpectation</span><span class="x"> </span><span class="kt">float64</span><span class="x">
	</span><span class="n">MultipleExpectation</span><span class="x"> </span><span class="p">[]</span><span class="kt">float64</span><span class="x">
</span><span class="p">}</span></code></pre></figure>

<p>It satisfies our needs with only four fields:</p>
<ul>
  <li><code class="highlighter-rouge">Features</code> is a slice of 64 bit float and this is perfect to represent input dimension,</li>
  <li><code class="highlighter-rouge">SingleRawExpectation</code> is a string and is filled by parser with input classification (in terms of belonging class),</li>
  <li><code class="highlighter-rouge">SingleExpectation</code> is a 64 bit float representation of the class which the pattern belongs,</li>
  <li><code class="highlighter-rouge">MultipleExpectation</code> is a slice of 64 bit float and it is used for multiple class classification problems;</li>
</ul>

<p>Why patterns? Because, our goal is to teach an ANN doing something, in this case counting, so… our patterns will be our binary number expressed as slice of 0 and 1. Immagine that we are giving a children a list of operation with numbers - in binary, poor little child. Anyway, this is to say: that child in a way or in another (definitely in another) will learn how to sum integer.</p>

<p>Who gives these number? The function <code class="highlighter-rouge">CreateRandomPatternArray(d, k)</code> that actually return a slice of <code class="highlighter-rouge">Pattern</code> (binary number). Perfect! We have numbers!</p>

<h5 id="neuron---code">Neuron - <a href="https://github.com/made2591/go-perceptron-go/blob/master/model/neural/neuronUnit.go">code</a></h5>
<p>The <code class="highlighter-rouge">NeuronUnit</code> struct represent a single computation unit. Look at the code:</p>

<figure class="highlight"><pre><code class="language-golang" data-lang="golang"><span class="c">// NeuronUnit struct represents a simple NeuronUnit network with a slice of n weights.</span><span class="x">
</span><span class="k">type</span><span class="x"> </span><span class="n">NeuronUnit</span><span class="x"> </span><span class="k">struct</span><span class="x"> </span><span class="p">{</span><span class="x">
	</span><span class="n">Weights</span><span class="x"> </span><span class="p">[]</span><span class="kt">float64</span><span class="x">
	</span><span class="n">Bias</span><span class="x"> </span><span class="kt">float64</span><span class="x">
	</span><span class="n">Lrate</span><span class="x"> </span><span class="kt">float64</span><span class="x">
	</span><span class="n">Value</span><span class="x"> </span><span class="kt">float64</span><span class="x">
	</span><span class="n">Delta</span><span class="x"> </span><span class="kt">float64</span><span class="x">
</span><span class="p">}</span></code></pre></figure>

<p>A neuron corresponds to the simple binary perceptron originally proposed by Rosenblat. It is made of:</p>
<ul>
  <li><code class="highlighter-rouge">Weights</code>, a slice of 64 bit float to represent the way each dimensions of the pattern is modulated,</li>
  <li><code class="highlighter-rouge">Bias</code>, a 64 bit float that represents NeuronUnit natural propensity to spread signal,</li>
  <li><code class="highlighter-rouge">Lrate</code>, a 64 bit float that represents learning rate of neuron,</li>
  <li><code class="highlighter-rouge">MultipleExpectation</code>, a 64 bit float that represents the desired value when I load the input pattner into network in Multi NeuralLayer Perceptron,</li>
  <li><code class="highlighter-rouge">Delta</code>, a 64 bit float that mantains error during execution of training algorithm (later);</li>
</ul>

<p>Again, every neuron of Elmann is a neuron in our neural child (what?!). Next step</p>

<h5 id="again-perceptrons">Again perceptrons?</h5>
<p>As you know, the single perceptron schema is implemented by a single neuron. The easiest way to implement this simple classifier is to establish a threshold function, insert it into the neuron, combine the values (eventually using different weights for each of them) that describe the stimulus in a single value, provide this value to the neuron and see what it returns in output. The schema show how it works:</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/6/60/ArtificialNeuronModel_english.png" alt="perceptron" /></p>

<p>We know that multilayer neural networks are a combo of element like the one shown above etc. Thus, in what is different an Elman network? Actually, as we said the only difference is the presence of a context layer - yes, the training algorithm is the back propagation as the one explained for perceptron (<strong>almost</strong>). Let’s say that an Elmann network is a three-layer network with the addition of this set of <em>context units</em>. The middle (hidden) layer is connected to these context units fixed with a weight of one. At each time step, the input is fed-forward and a learning rule is applied. The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state, allowing it to perform such tasks as <em>sequence-prediction</em> that are beyond the power of a standard multilayer perceptron.</p>

<h6 id="back-propagation---differences">Back propagation - differences</h6>
<p>Ok, the code is almost the same as defined for Perceptron, available <a href="https://github.com/made2591/go-perceptron-go/blob/master/model/neural/multiLayerNetwork.go">here</a>. Actually, it is because in the end the only difference is that we want the neural network to be able to store the neural hidden values at every step in the context. In fact, to preserve the MLPerceptron struct I extended the two method involved in training, <code class="highlighter-rouge">BackPropagate</code> and <code class="highlighter-rouge">Execute</code>, with an optional argument (options …int).</p>

<figure class="highlight"><pre><code class="language-golang" data-lang="golang"><span class="c">// BackPropagation algorithm for assisted learning. Convergence is not guaranteed and very slow.</span><span class="x">
</span><span class="c">// Use as a stop criterion the average between previous and current errors and a maximum number of iterations.</span><span class="x">
</span><span class="c">// [mlp:MultiLayerNetwork] input value [s:Pattern] input value (scaled between 0 and 1)</span><span class="x">
</span><span class="c">// [o:[]float64] expected output value (scaled between 0 and 1)</span><span class="x">
</span><span class="c">// return [r:float64] delta error between generated output and expected output</span><span class="x">
</span><span class="k">func</span><span class="x"> </span><span class="n">BackPropagate</span><span class="p">(</span><span class="n">mlp</span><span class="x"> </span><span class="o">*</span><span class="n">MultiLayerNetwork</span><span class="p">,</span><span class="x"> </span><span class="n">s</span><span class="x"> </span><span class="o">*</span><span class="n">Pattern</span><span class="p">,</span><span class="x"> </span><span class="n">o</span><span class="x"> </span><span class="p">[]</span><span class="kt">float64</span><span class="p">,</span><span class="x"> </span><span class="n">options</span><span class="x"> </span><span class="o">...</span><span class="kt">int</span><span class="p">)</span><span class="x"> </span><span class="p">(</span><span class="n">r</span><span class="x"> </span><span class="kt">float64</span><span class="p">)</span><span class="x"> </span><span class="p">{</span><span class="x">

    </span><span class="k">var</span><span class="x"> </span><span class="n">no</span><span class="x"> </span><span class="p">[]</span><span class="kt">float64</span><span class="p">;</span><span class="x">
    </span><span class="c">// execute network with pattern passed over each level to output</span><span class="x">
    </span><span class="k">if</span><span class="x"> </span><span class="nb">len</span><span class="p">(</span><span class="n">options</span><span class="p">)</span><span class="x"> </span><span class="o">==</span><span class="x"> </span><span class="m">1</span><span class="x"> </span><span class="p">{</span><span class="x">
        </span><span class="n">no</span><span class="x"> </span><span class="o">=</span><span class="x"> </span><span class="n">Execute</span><span class="p">(</span><span class="n">mlp</span><span class="p">,</span><span class="x"> </span><span class="n">s</span><span class="p">,</span><span class="x"> </span><span class="n">options</span><span class="p">[</span><span class="m">0</span><span class="p">])</span><span class="x">
    </span><span class="p">}</span><span class="x"> </span><span class="k">else</span><span class="x"> </span><span class="p">{</span><span class="x">
        </span><span class="n">no</span><span class="x"> </span><span class="o">=</span><span class="x"> </span><span class="n">Execute</span><span class="p">(</span><span class="n">mlp</span><span class="p">,</span><span class="x"> </span><span class="n">s</span><span class="p">)</span><span class="x">
    </span><span class="p">}</span><span class="x">

    </span><span class="o">...</span><span class="x">

        </span><span class="c">// copy hidden output to context</span><span class="x">
        </span><span class="k">if</span><span class="x"> </span><span class="n">k</span><span class="x"> </span><span class="o">==</span><span class="x"> </span><span class="m">1</span><span class="x"> </span><span class="o">&amp;&amp;</span><span class="x"> </span><span class="nb">len</span><span class="p">(</span><span class="n">options</span><span class="p">)</span><span class="x"> </span><span class="o">&gt;</span><span class="x"> </span><span class="m">0</span><span class="x"> </span><span class="o">&amp;&amp;</span><span class="x"> </span><span class="n">options</span><span class="p">[</span><span class="m">0</span><span class="p">]</span><span class="x"> </span><span class="o">==</span><span class="x"> </span><span class="m">1</span><span class="x"> </span><span class="p">{</span><span class="x">

            </span><span class="k">for</span><span class="x"> </span><span class="n">z</span><span class="x"> </span><span class="o">:=</span><span class="x"> </span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">Features</span><span class="p">);</span><span class="x"> </span><span class="n">z</span><span class="x"> </span><span class="o">&lt;</span><span class="x"> </span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="m">0</span><span class="p">]</span><span class="o">.</span><span class="n">Length</span><span class="p">;</span><span class="x"> </span><span class="n">z</span><span class="o">++</span><span class="x"> </span><span class="p">{</span><span class="x">

                </span><span class="c">// save output of hidden layer to context</span><span class="x">
                </span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="m">0</span><span class="p">]</span><span class="o">.</span><span class="n">NeuronUnits</span><span class="p">[</span><span class="n">z</span><span class="p">]</span><span class="o">.</span><span class="n">Value</span><span class="x"> </span><span class="o">=</span><span class="x"> </span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">NeuronUnits</span><span class="p">[</span><span class="n">z</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">Features</span><span class="p">)]</span><span class="o">.</span><span class="n">Value</span><span class="x">

            </span><span class="p">}</span><span class="x">

        </span><span class="p">}</span><span class="x">

    </span><span class="o">...</span></code></pre></figure>

<p>and during the execution part of the network this means propagate to context:</p>

<figure class="highlight"><pre><code class="language-golang" data-lang="golang"><span class="c">// save output of hidden layer to context if nextwork is RECURRENT</span><span class="x">
</span><span class="k">if</span><span class="x"> </span><span class="n">k</span><span class="x"> </span><span class="o">==</span><span class="x"> </span><span class="m">1</span><span class="x"> </span><span class="o">&amp;&amp;</span><span class="x"> </span><span class="nb">len</span><span class="p">(</span><span class="n">options</span><span class="p">)</span><span class="x"> </span><span class="o">&gt;</span><span class="x"> </span><span class="m">0</span><span class="x"> </span><span class="o">&amp;&amp;</span><span class="x"> </span><span class="n">options</span><span class="p">[</span><span class="m">0</span><span class="p">]</span><span class="x"> </span><span class="o">==</span><span class="x"> </span><span class="m">1</span><span class="x"> </span><span class="p">{</span><span class="x">

    </span><span class="k">for</span><span class="x"> </span><span class="n">z</span><span class="x"> </span><span class="o">:=</span><span class="x"> </span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">Features</span><span class="p">);</span><span class="x"> </span><span class="n">z</span><span class="x"> </span><span class="o">&lt;</span><span class="x"> </span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="m">0</span><span class="p">]</span><span class="o">.</span><span class="n">Length</span><span class="p">;</span><span class="x"> </span><span class="n">z</span><span class="o">++</span><span class="x"> </span><span class="p">{</span><span class="x">

        </span><span class="n">log</span><span class="o">.</span><span class="n">WithFields</span><span class="p">(</span><span class="n">log</span><span class="o">.</span><span class="n">Fields</span><span class="p">{</span><span class="x">
            </span><span class="s">"level"</span><span class="x">				</span><span class="o">:</span><span class="x"> </span><span class="s">"debug"</span><span class="p">,</span><span class="x">
            </span><span class="s">"len z"</span><span class="x"> 			</span><span class="o">:</span><span class="x"> </span><span class="n">z</span><span class="p">,</span><span class="x">
            </span><span class="s">"s.Features"</span><span class="x">		</span><span class="o">:</span><span class="x"> </span><span class="n">s</span><span class="o">.</span><span class="n">Features</span><span class="p">,</span><span class="x">
            </span><span class="s">"len(s.Features)"</span><span class="x"> </span><span class="o">:</span><span class="x"> </span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">Features</span><span class="p">),</span><span class="x">
            </span><span class="s">"len mlp.NeuralLayers[0].NeuronUnits"</span><span class="x"> </span><span class="o">:</span><span class="x"> </span><span class="nb">len</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="m">0</span><span class="p">]</span><span class="o">.</span><span class="n">NeuronUnits</span><span class="p">),</span><span class="x">
            </span><span class="s">"len mlp.NeuralLayers[k].NeuronUnits"</span><span class="x"> </span><span class="o">:</span><span class="x"> </span><span class="nb">len</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">NeuronUnits</span><span class="p">),</span><span class="x">
        </span><span class="p">})</span><span class="o">.</span><span class="n">Debug</span><span class="p">(</span><span class="s">"Save output of hidden layer to context."</span><span class="p">)</span><span class="x">

        </span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="m">0</span><span class="p">]</span><span class="o">.</span><span class="n">NeuronUnits</span><span class="p">[</span><span class="n">z</span><span class="p">]</span><span class="o">.</span><span class="n">Value</span><span class="x"> </span><span class="o">=</span><span class="x"> </span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">NeuronUnits</span><span class="p">[</span><span class="n">z</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">Features</span><span class="p">)]</span><span class="o">.</span><span class="n">Value</span><span class="x">

    </span><span class="p">}</span><span class="x">

</span><span class="p">}</span></code></pre></figure>

<h5 id="where-is-the-context">Where is the context</h5>
<p>As you most probably noticed, I made a magic trick: to avoid create a new neural network struct, I used the input layer as layer to also store the context layer. That is the reason I loop with index z starting from len(s.Features) in both the <code class="highlighter-rouge">BackPropagate</code> and <code class="highlighter-rouge">Execute</code>.</p>

<p>How to run it?</p>

<figure class="highlight"><pre><code class="language-sh" data-lang="sh">go get github.com/made2591/go-perceptron-go
<span class="nb">cd</span> <span class="nv">$GOPATH</span>/src/made2591/go-perceptron-go
go run main.go</code></pre></figure>

<p>Thank you everybody for reading!</p>


<div class="comments">
   <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'made2591';

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    

</div>

<hr/>


  
    <div class="post-navs row">
      
        <div class="col-md-6 post-nav">

          <h3 class="section-header">
            Older
            <span class="text-muted"> &middot; </span>
            <a href="/archive">View Archive (35)</a>
          </h3>

          <h2 class="post-title-link"><a href="/posts/act-r-part-II">ACT-R by John R. Anderson - Part II</a></h2>
          <h3 id="introduction">Introduction</h3>
<p>In my <a href="https://made2591.github.io/posts/act-r-part-I">previous post</a> I wrote about the cognitive architecture ACT-R, mainly putting together what I learnt by research over the topic. In this post, I would like to go more in depth about how ACT-R works, the concepts behind and try to provide my interpretation of some technical examples, regarding coding of the modeling and everything related.</p>


        </div>
      
      

        <div class="col-md-6 post-nav">
          <h3 class="section-header">
            Newer
            
          </h3>

          <h2 class="post-title-link"><a href="/posts/act-r-part-III">ACT-R by John R. Anderson - Part III</a></h2>
          

        </div>
      
    </div>
  



          <div id="footer">
<hr/>
  <div class="pull-left">
    &copy;2019.
    Built with <a href="http://jekyllrb.com/">Jekyll</a> and
    <a href="https://github.com/kirqe/autm-rb">Autm-rb</a>. Thanks to <a href="https://github.com/kirqe">kirqe</a>.
  </div>

  <div class="pull-right">
    <span class="credits pull-right social">
      
        
          <a href="https://github.com/made2591/" target="_blank"><i class="fa fa-github"></i></a>
        
      
        
          <a href="https://twitter.com/made2591" target="_blank"><i class="fa fa-twitter"></i></a>
        
      
        
          <a href="https://linkedin.com/in/mmadeddu" target="_blank"><i class="fa fa-linkedin"></i></a>
        
      
        
          <a href="https://www.facebook.com/matteo.madeddu" target="_blank"><i class="fa fa-facebook"></i></a>
        
      
    </span>
  </div>
</div>

        </section>
    </div>
    </div>
    <script src="/public/javascripts/jquery.min.js"></script>
    <script src="/public/javascripts/bootstrap.min.js"></script>
    <!-- Place your <script> tags here. -->

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script id="dsq-count-scr" src="//made2591-github-io.disqus.com/count.js" async></script>

<!-- disqus comment counter -->
<script type="text/javascript">
	/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'made2591-github-io';

	/* * * DON'T EDIT BELOW THIS LINE * * */
	(function () {
	    var s = document.createElement('script'); s.async = true;
	    s.type = 'text/javascript';
	    s.src = '//' + disqus_shortname + '.disqus.com/count.js';
	    (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
	}());
</script>
<!-- /disqus comment counter -->

<!-- google analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-111283556-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-111283556-1');
</script>
<!-- /google analytics -->

  </body>
</html>

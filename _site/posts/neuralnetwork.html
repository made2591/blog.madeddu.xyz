<!DOCTYPE html>
<html>
  <head>
    <!-- Google Tag Manager -->
    <!--<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-M8WBXH7');</script>-->
    <!-- End Google Tag Manager -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,300,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href=/public/stylesheets/bs.css?random=@Environment.TickCount>
    <link rel="stylesheet" href=/public/stylesheets/styles.css?random=@Environment.TickCount>
    <link rel="stylesheet" href=/public/stylesheets/pygment_trac.css?random=@Environment.TickCount>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.1/css/font-awesome.min.css">
    <link rel="canonical" href="made2591.github.io/posts/neuralnetwork">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
      <link rel="icon" type="image/x-icon"  href="/favicon.ico" />
    <title>Build a multilayer perceptron with Golang | Matteo Madeddu</title>
    <!--[if lt IE 9]>

      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
  </head>
  <body>
    <!-- Google Tag Manager (noscript) -->
    <!--<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-M8WBXH7"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->
    <div class="wrapper">
        <section>
          <div id="header">
  <h1>
    <a id="sitename" href="">Matteo Madeddu</a>
  </h1>

  <!-- Add something about you in p tag-->
  <p>Mac OS lover, Docker fan, Go explorer, Python geek, Trello addicted.</p>
  <hr/>

  <span class="credits pull-left">
    
      <a href="/">Home</a>  | 
    
      <a href="/blog">Blog</a>  | 
    
      <a href="/archive">Archive</a>  | 
    
      <a href="/about">About</a>  | 
    
      <a href="/matteo_madeddu_cv.pdf">Resume</a>  | 
    
      <a href="/quotes">Quotes</a> 
    
  </span>

  <span class="credits pull-right social">
    
      
        <a href="https://github.com/made2591/" target="_blank"><i class="fa fa-github"></i></a>
      
    
      
        <a href="https://twitter.com/made2591" target="_blank"><i class="fa fa-twitter"></i></a>
      
    
      
        <a href="https://linkedin.com/in/mmadeddu" target="_blank"><i class="fa fa-linkedin"></i></a>
      
    
      
        <a href="https://www.facebook.com/matteo.madeddu" target="_blank"><i class="fa fa-facebook"></i></a>
      
    
  </span>


</div>


          <div class="post-title">
    <h1>Build a multilayer perceptron with Golang</h1>
    <p class="text-muted">
    


    20 Dec 2017
    
     | <i class="fa fa-comment"></i> <a class="text-muted" href="/posts/neuralnetwork/#disqus_thread"></a>
    

    
      | <i class="fa fa-tag"></i>
      
        <a class="text-muted" href="/tags/#coding ">coding</a>,
      
        <a class="text-muted" href="/tags/#golang ">golang</a>,
      
        <a class="text-muted" href="/tags/#ann ">ann</a>,
      
        <a class="text-muted" href="/tags/#perceptron ">perceptron</a>,
      
        <a class="text-muted" href="/tags/#classifier ">classifier</a>,
      
        <a class="text-muted" href="/tags/#neural ">neural</a>,
      
        <a class="text-muted" href="/tags/#networks ">networks</a>
      
    
  </p>
</div>

  <h3 id="history">History</h3>
<p>We can date the birth of artificial neural networks in 1958, with the introduction of Perceptron <sup id="fnref:rosen"><a href="#fn:rosen" class="footnote">1</a></sup> by Frank Rosenblatt. It was the first algorithm created to reproduce the biological neuron. Conceptually, the easier perceptron that you might think of is made of a single neuron: when it’s exposed to a stimulus, it provides a binary response, just as would a biological neuron.</p>

<p><img src="https://pbs.twimg.com/media/DPtxHXKW4AEcLyc.jpg" alt="ann" /></p>

<p>This model differs greatly from the neural network involving billions of neurons in a biological brain. Shortly after his birth, the researchers showed the world the problems of Perceptron: in fact, it was quickly proved that perceptrons could not be trained to recognize many classes of input patterns. To get a more powerful network, it was necessary to take advantage of multiple level of units and create a multilayers perceptron, with more intermediates neurons used to solve linearly separable<sup id="fnref:linsep"><a href="#fn:linsep" class="footnote">2</a></sup> subproblems, whose outputs were combined together by the final level to provide a concrete response to original input problem. Even though the Perceptron was just a simple but severely limited binary classifier, it introduced a great innovation: the idea to simulate the basic computational unit of a complex biological system that exists in nature.</p>

<h3 id="theory">Theory</h3>
<p>Fundamentally, a neural network is nothing more than a really good function approximator — I mean, you give a trained network as an input vector, it performs a series of operations, and it produces an output vector. To train an ann to estimate an unknown function, the process is really simple: you have to get a training set - a collection of data points - that the network will learn from and generalize on to make future inferences. In a multilayer perceptron data points are forwarded through the network layer-by-layer until they reach the final layer. The final layer’s activations are the predictions that the network actually makes. In this article, I describe how I built with Golang my own perceptron - and then a multilayer perceptron. Let first talk about the representation of the input: all the example codes are from my <a href="https://github.com/made2591/go-perceptron-go">go-perceptron-go</a> repository.</p>

<h4 id="base-structures---code">Base structures - <a href="https://github.com/made2591/go-perceptron-go/tree/master/model/neural">code</a></h4>
<p>To create a neural network, the first thing you have to do is dealing with the definition of data structures. I create a <code class="highlighter-rouge">neural</code> package to collect all files related to architecture structure and elements.</p>

<h5 id="pattern---code">Pattern - <a href="https://github.com/made2591/go-perceptron-go/blob/master/model/neural/pattern.go">code</a></h5>
<p>The <code class="highlighter-rouge">Pattern</code> struct represent a single input to the <code class="highlighter-rouge">Perceptron</code> struct. Look at the code:</p>

<figure class="highlight"><pre><code class="language-golang" data-lang="golang"><span class="c">// Pattern struct represents one pattern with dimensions and desired value</span><span class="x">
</span><span class="k">type</span><span class="x"> </span><span class="n">Pattern</span><span class="x"> </span><span class="k">struct</span><span class="x"> </span><span class="p">{</span><span class="x">
	</span><span class="n">Features</span><span class="x"> </span><span class="p">[]</span><span class="kt">float64</span><span class="x">
	</span><span class="n">SingleRawExpectation</span><span class="x"> </span><span class="kt">string</span><span class="x">
	</span><span class="n">SingleExpectation</span><span class="x"> </span><span class="kt">float64</span><span class="x">
	</span><span class="n">MultipleExpectation</span><span class="x"> </span><span class="p">[]</span><span class="kt">float64</span><span class="x">
</span><span class="p">}</span></code></pre></figure>

<p>It satisfies our needs with only four fields:</p>
<ul>
  <li><code class="highlighter-rouge">Features</code> is a slice of 64 bit float and this is perfect to represent input dimension,</li>
  <li><code class="highlighter-rouge">SingleRawExpectation</code> is a string and is filled by parser with input classification (in terms of belonging class),</li>
  <li><code class="highlighter-rouge">SingleExpectation</code> is a 64 bit float representation of the class which the pattern belongs,</li>
  <li><code class="highlighter-rouge">MultipleExpectation</code> is a slice of 64 bit float and it is used for multiple class classification problems;</li>
</ul>

<h5 id="neuron---code">Neuron - <a href="https://github.com/made2591/go-perceptron-go/blob/master/model/neural/neuronUnit.go">code</a></h5>
<p>The <code class="highlighter-rouge">NeuronUnit</code> struct represent a single computation unit. Look at the code:</p>

<figure class="highlight"><pre><code class="language-golang" data-lang="golang"><span class="c">// NeuronUnit struct represents a simple NeuronUnit network with a slice of n weights.</span><span class="x">
</span><span class="k">type</span><span class="x"> </span><span class="n">NeuronUnit</span><span class="x"> </span><span class="k">struct</span><span class="x"> </span><span class="p">{</span><span class="x">
	</span><span class="n">Weights</span><span class="x"> </span><span class="p">[]</span><span class="kt">float64</span><span class="x">
	</span><span class="n">Bias</span><span class="x"> </span><span class="kt">float64</span><span class="x">
	</span><span class="n">Lrate</span><span class="x"> </span><span class="kt">float64</span><span class="x">
	</span><span class="n">Value</span><span class="x"> </span><span class="kt">float64</span><span class="x">
	</span><span class="n">Delta</span><span class="x"> </span><span class="kt">float64</span><span class="x">
</span><span class="p">}</span></code></pre></figure>

<p>A neuron corresponds to the simple binary perceptron originally proposed by Rosenblat. It is made of:</p>
<ul>
  <li><code class="highlighter-rouge">Weights</code>, a slice of 64 bit float to represent the way each dimensions of the pattern is modulated,</li>
  <li><code class="highlighter-rouge">Bias</code>, a 64 bit float that represents NeuronUnit natural propensity to spread signal,</li>
  <li><code class="highlighter-rouge">Lrate</code>, a 64 bit float that represents learning rate of neuron,</li>
  <li><code class="highlighter-rouge">MultipleExpectation</code>, a 64 bit float that represents the desired value when I load the input pattner into network in Multi NeuralLayer Perceptron,</li>
  <li><code class="highlighter-rouge">Delta</code>, a 64 bit float that mantains error during execution of training algorithm (later);</li>
</ul>

<h5 id="perceptron">Perceptron</h5>
<p>As I said, the single perceptron schema is implemented by a single neuron. The easiest way to implement this simple classifier is to establish a threshold function, insert it into the neuron, combine the values (eventually using different weights for each of them) that describe the stimulus in a single value, provide this value to the neuron and see what it returns in output. The schema show how it works:</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/6/60/ArtificialNeuronModel_english.png" alt="perceptron" /></p>

<h5 id="metric">Metric</h5>
<p>Why <em>weights</em>? What does it mean the expression <em>dimension modulation</em> of the the input? Well, training conceptually is “the process of learning the skills you need to do a particular job or activity”. But how do you know if you’re getting better, or if you are learning the skills you need? Of course, you need a metric of how good or bad you’re doing. Also in ANN there’s a metric generally called <em>cost function</em>. Suppose we want to change a certain <em>wi</em> weight of the network. More or less, the cost function looks at the function the network has inferred and uses it to estimate values for the data points in the training set. The difference between the outputs of the network and the training set data points are the main values for the cost function. When training your network, the goal is to get the value of this cost function as low as possible. The most basic of the training algorithms is the <em>gradient descent</em>.
Suppose we can calculate the error <em>E</em> according to the variation of the weight value <em>wi</em>: we are therefore able to draw the graph in a graph like the one in the figure.</p>

<p align="center"><img src="https://image.ibb.co/jJH2em/graph.png" alt="perceptron" style="width: 250px; marker-top: -10px;" /></p>

<p>Therefore, if we calculate the derivative of this function, we can understand how the variation of the weight makes a positive or negative contribution to the error. In practice, whatever the derived value, we can use a single weight correction function that decrease the involved weight of derived quantity (modulated by learning rate). Despite the fact that it’s quite impossible, for any network or cost function, to be truly convex, the gradient descent follows the derivatives computed for each neuron unit to essentially “roll” down the slope until it finds its way to the center - as close as possible to the <em>global minimum</em>. Before continuing, let’s take a step back.</p>

<h5 id="why-multilayer-the-linearly-separable-problems">Why multilayer? The linearly separable problems</h5>
<p>The problem with the binary perceptron made with a single neuron is the inability to handle non-linearly separable problems: these kind of problems are the ones in which, in other words, it’s impossible to define an hyperplane able to separate, in the vector space of the inputs, those that require a positive output from those requiring a negative output. An example of three non-collinear points belonging to two different classes (‘<em>+</em>’ and ‘<em>-</em>’) are always linearly separable in two dimensions. This is illustrated by the first three examples in the following figure:</p>

<p align="center"><img src="https://image.ibb.co/coBkX6/linear.png" alt="perceptron" style="width: 250px; marker-top: -10px;" /></p>

<p>However, not all sets of four points, no three collinear, are linearly separable in two dimensions. The fourth image would need two straight lines and thus is not linearly separable. This is the main reason scientist start working with multilayers at the very beginning. Let’s move one step forward, introducing the <code class="highlighter-rouge">NeuralLayer</code> struct.</p>

<h5 id="neural-layer---code">Neural Layer - <a href="https://github.com/made2591/go-perceptron-go/blob/master/model/neural/neuralLayer.go">code</a></h5>
<p>The <code class="highlighter-rouge">NeuralLayer</code> struct represents a network layer with a slice of <em>n</em> <code class="highlighter-rouge">NeuronUnits</code>.</p>

<figure class="highlight"><pre><code class="language-golang" data-lang="golang"><span class="k">type</span><span class="x"> </span><span class="n">NeuralLayer</span><span class="x"> </span><span class="k">struct</span><span class="x"> </span><span class="p">{</span><span class="x">
	</span><span class="n">NeuronUnits</span><span class="x"> </span><span class="p">[]</span><span class="n">NeuronUnit</span><span class="x">
	</span><span class="n">Length</span><span class="x"> </span><span class="kt">int</span><span class="x">
</span><span class="p">}</span></code></pre></figure>

<p>where:</p>
<ul>
  <li><code class="highlighter-rouge">NeuronUnits</code> represents NeuronUnits in layer,</li>
  <li><code class="highlighter-rouge">Length</code> represents number of NeuronUnit in layer;</li>
</ul>

<p>Now that we are able to build layers of neurons, we can define the <code class="highlighter-rouge">MultiLayerNetwork</code> struct.</p>

<h5 id="multilayer-perceptron---code">Multilayer Perceptron - <a href="https://github.com/made2591/go-perceptron-go/blob/master/model/neural/multiLayerNetwork.go">code</a></h5>

<figure class="highlight"><pre><code class="language-golang" data-lang="golang"><span class="k">type</span><span class="x"> </span><span class="n">MultiLayerNetwork</span><span class="x"> </span><span class="k">struct</span><span class="x"> </span><span class="p">{</span><span class="x">
	</span><span class="n">L_rate</span><span class="x"> </span><span class="kt">float64</span><span class="x">
	</span><span class="n">NeuralLayers</span><span class="x"> </span><span class="p">[]</span><span class="n">NeuralLayer</span><span class="x">
	</span><span class="n">T_func</span><span class="x"> </span><span class="n">transferFunction</span><span class="x">
	</span><span class="n">T_func_d</span><span class="x"> </span><span class="n">transferFunction</span><span class="x">
</span><span class="p">}</span></code></pre></figure>

<p>where:</p>
<ul>
  <li><code class="highlighter-rouge">NeuralLayers</code> represents layer of neurons,</li>
  <li><code class="highlighter-rouge">Length</code> represents learning rate of neuron,</li>
  <li><code class="highlighter-rouge">T_func</code> and <code class="highlighter-rouge">T_func_d</code> represents the transferFunction and its derivative;</li>
</ul>

<p>Inside the <code class="highlighter-rouge">MultiLayerNetwork</code> struct there’s an algorithm to create multilayer perceptron: if you pass a struct with <code class="highlighter-rouge">NeuralLayers</code> [4, 3, 3], you can define a network struct with 3 layer: input, hidden, output, with respectively 4, 3 and 3 neurons, as shown in the figure below.</p>

<p><img src="https://image.ibb.co/j9jgpm/first_example_copia.png" alt="perceptron" /></p>

<p>The piece of code that handle network creation is the following:</p>

<figure class="highlight"><pre><code class="language-golang" data-lang="golang"><span class="c">// ... the following is in the neuralLayer.go</span><span class="x">

</span><span class="c">// PrepareLayer create a NeuralLayer with n NeuronUnits inside</span><span class="x">
</span><span class="c">// [n:int] is an int that specifies the number of neurons in the NeuralLayer</span><span class="x">
</span><span class="c">// [p:int] is an int that specifies the number of neurons in the previous NeuralLayer</span><span class="x">
</span><span class="c">// It returns a NeuralLayer object</span><span class="x">
</span><span class="k">func</span><span class="x"> </span><span class="n">PrepareLayer</span><span class="p">(</span><span class="n">n</span><span class="x"> </span><span class="kt">int</span><span class="p">,</span><span class="x"> </span><span class="n">p</span><span class="x"> </span><span class="kt">int</span><span class="p">)</span><span class="x"> </span><span class="p">(</span><span class="n">l</span><span class="x"> </span><span class="n">NeuralLayer</span><span class="p">)</span><span class="x"> </span><span class="p">{</span><span class="x">

	</span><span class="n">l</span><span class="x"> </span><span class="o">=</span><span class="x"> </span><span class="n">NeuralLayer</span><span class="p">{</span><span class="n">NeuronUnits</span><span class="o">:</span><span class="x"> </span><span class="nb">make</span><span class="p">([]</span><span class="n">NeuronUnit</span><span class="p">,</span><span class="x"> </span><span class="n">n</span><span class="p">),</span><span class="x"> </span><span class="n">Length</span><span class="o">:</span><span class="x"> </span><span class="n">n</span><span class="p">}</span><span class="x">

	</span><span class="k">for</span><span class="x"> </span><span class="n">i</span><span class="x"> </span><span class="o">:=</span><span class="x"> </span><span class="m">0</span><span class="p">;</span><span class="x"> </span><span class="n">i</span><span class="x"> </span><span class="o">&lt;</span><span class="x"> </span><span class="n">n</span><span class="p">;</span><span class="x"> </span><span class="n">i</span><span class="o">++</span><span class="x"> </span><span class="p">{</span><span class="x">
		</span><span class="n">RandomNeuronInit</span><span class="p">(</span><span class="o">&amp;</span><span class="n">l</span><span class="o">.</span><span class="n">NeuronUnits</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="x"> </span><span class="n">p</span><span class="p">)</span><span class="x">
	</span><span class="p">}</span><span class="x">

	</span><span class="n">log</span><span class="o">.</span><span class="n">WithFields</span><span class="p">(</span><span class="n">log</span><span class="o">.</span><span class="n">Fields</span><span class="p">{</span><span class="x">
		</span><span class="s">"level"</span><span class="o">:</span><span class="x">   </span><span class="s">"info"</span><span class="p">,</span><span class="x">
		</span><span class="s">"msg"</span><span class="o">:</span><span class="x">     </span><span class="s">"multilayer perceptron init completed"</span><span class="p">,</span><span class="x">
		</span><span class="s">"neurons"</span><span class="o">:</span><span class="x"> </span><span class="nb">len</span><span class="p">(</span><span class="n">l</span><span class="o">.</span><span class="n">NeuronUnits</span><span class="p">),</span><span class="x">
		</span><span class="s">"lengthPreviousLayer"</span><span class="o">:</span><span class="x"> </span><span class="n">l</span><span class="o">.</span><span class="n">Length</span><span class="p">,</span><span class="x">
	</span><span class="p">})</span><span class="o">.</span><span class="n">Info</span><span class="p">(</span><span class="s">"Complete NeuralLayer init."</span><span class="p">)</span><span class="x">

	</span><span class="k">return</span><span class="x">

</span><span class="p">}</span><span class="x">

</span><span class="c">// ... the following is in the multiLayerNetwork.go</span><span class="x">

</span><span class="c">// PrepareMLPNet create a multi layer Perceptron neural network.</span><span class="x">
</span><span class="c">// [l:[]int] is an int array with layers neurons number [input, ..., output]</span><span class="x">
</span><span class="c">// [lr:int] is the learning rate of neural network</span><span class="x">
</span><span class="c">// [tr:transferFunction] is a transfer function</span><span class="x">
</span><span class="c">// [tr:transferFunction] the respective transfer function derivative</span><span class="x">
</span><span class="k">func</span><span class="x"> </span><span class="n">PrepareMLPNet</span><span class="p">(</span><span class="n">l</span><span class="x"> </span><span class="p">[]</span><span class="kt">int</span><span class="p">,</span><span class="x"> </span><span class="n">lr</span><span class="x"> </span><span class="kt">float64</span><span class="p">,</span><span class="x"> </span><span class="n">tf</span><span class="x"> </span><span class="n">transferFunction</span><span class="p">,</span><span class="x"> </span><span class="n">trd</span><span class="x"> </span><span class="n">transferFunction</span><span class="p">)</span><span class="x"> </span><span class="p">(</span><span class="n">mlp</span><span class="x"> </span><span class="n">MultiLayerNetwork</span><span class="p">)</span><span class="x"> </span><span class="p">{</span><span class="x">

	</span><span class="c">// setup learning rate and transfer function</span><span class="x">
	</span><span class="n">mlp</span><span class="o">.</span><span class="n">L_rate</span><span class="x"> </span><span class="o">=</span><span class="x"> </span><span class="n">lr</span><span class="x">
	</span><span class="n">mlp</span><span class="o">.</span><span class="n">T_func</span><span class="x"> </span><span class="o">=</span><span class="x"> </span><span class="n">tf</span><span class="x">
	</span><span class="n">mlp</span><span class="o">.</span><span class="n">T_func_d</span><span class="x"> </span><span class="o">=</span><span class="x"> </span><span class="n">trd</span><span class="x">

	</span><span class="c">// setup layers</span><span class="x">
	</span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="x"> </span><span class="o">=</span><span class="x"> </span><span class="nb">make</span><span class="p">([]</span><span class="n">NeuralLayer</span><span class="p">,</span><span class="x"> </span><span class="nb">len</span><span class="p">(</span><span class="n">l</span><span class="p">))</span><span class="x">

	</span><span class="c">// for each layers specified</span><span class="x">
	</span><span class="k">for</span><span class="x"> </span><span class="n">il</span><span class="p">,</span><span class="x"> </span><span class="n">ql</span><span class="x"> </span><span class="o">:=</span><span class="x"> </span><span class="k">range</span><span class="x"> </span><span class="n">l</span><span class="x"> </span><span class="p">{</span><span class="x">

		</span><span class="c">// if it is not the first</span><span class="x">
		</span><span class="k">if</span><span class="x"> </span><span class="n">il</span><span class="x"> </span><span class="o">!=</span><span class="x"> </span><span class="m">0</span><span class="x"> </span><span class="p">{</span><span class="x">

			</span><span class="c">// prepare the GENERIC layer with specific dimension and correct number of links for each NeuronUnits</span><span class="x">
			</span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="n">il</span><span class="p">]</span><span class="x"> </span><span class="o">=</span><span class="x"> </span><span class="n">PrepareLayer</span><span class="p">(</span><span class="n">ql</span><span class="p">,</span><span class="x"> </span><span class="n">l</span><span class="p">[</span><span class="n">il</span><span class="o">-</span><span class="m">1</span><span class="p">])</span><span class="x">

		</span><span class="p">}</span><span class="x"> </span><span class="k">else</span><span class="x"> </span><span class="p">{</span><span class="x">

			</span><span class="c">// prepare the INPUT layer with specific dimension and No links to previous.</span><span class="x">
			</span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="n">il</span><span class="p">]</span><span class="x"> </span><span class="o">=</span><span class="x"> </span><span class="n">PrepareLayer</span><span class="p">(</span><span class="n">ql</span><span class="p">,</span><span class="x"> </span><span class="m">0</span><span class="p">)</span><span class="x">

		</span><span class="p">}</span><span class="x">

	</span><span class="p">}</span><span class="x">

	</span><span class="n">log</span><span class="o">.</span><span class="n">WithFields</span><span class="p">(</span><span class="n">log</span><span class="o">.</span><span class="n">Fields</span><span class="p">{</span><span class="x">
		</span><span class="s">"level"</span><span class="o">:</span><span class="x">     </span><span class="s">"info"</span><span class="p">,</span><span class="x">
		</span><span class="s">"msg"</span><span class="o">:</span><span class="x">       </span><span class="s">"multilayer perceptron init completed"</span><span class="p">,</span><span class="x">
		</span><span class="s">"layers"</span><span class="o">:</span><span class="x">  </span><span class="nb">len</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">),</span><span class="x">
		</span><span class="s">"learningRate: "</span><span class="o">:</span><span class="x"> </span><span class="n">mlp</span><span class="o">.</span><span class="n">L_rate</span><span class="p">,</span><span class="x">
	</span><span class="p">})</span><span class="o">.</span><span class="n">Info</span><span class="p">(</span><span class="s">"Complete Multilayer Perceptron init."</span><span class="p">)</span><span class="x">

	</span><span class="k">return</span><span class="x">

</span><span class="p">}</span></code></pre></figure>

<p>For classification problems the input layers has to be define with a number of neurons that match features of pattern shown to network. Of course, the output layer should have a number of unit equals to the number of class in training set.</p>

<p><strong>NOTE</strong>: from the architectural point of view an interesting theorem guarantee that <em>given a sufficient number of hidden units, everything that can be solved by a multilayer network at n levels can also be solved by a two-level network</em>. Therefore in examples we will limit ourselves to using only two levels.</p>

<h4 id="backpropagation-algorithm---code">BackPropagation Algorithm - <a href="https://github.com/made2591/go-perceptron-go/blob/master/model/neural/multiLayerNetwork.go">code</a></h4>
<p>The learning algorithm can be divided into two phases: propagation and weight update.</p>

<h5 id="propagation---1-of-2">Propagation - 1 of 2</h5>
<p>Each propagation involves the following steps:</p>

<ul>
  <li>the <em>propagation</em> forward through the network to generate the output value(s) is done by <code class="highlighter-rouge">Execute</code> function,</li>
  <li>the calculation of the cost (error term) is done here at the very beginning of the <code class="highlighter-rouge">BackPropagate</code> function,</li>
  <li>the propagation of the output activations <strong>back</strong> through the network, using the training pattern target in order to generate the deltas (the difference between the targeted and actual output values) of all output and hidden neurons, done of coure <code class="highlighter-rouge">BackPropagate</code> function;</li>
</ul>

<p>First, let’s have a look to the <code class="highlighter-rouge">Execute</code> function.</p>

<figure class="highlight"><pre><code class="language-golang" data-lang="golang"><span class="c">// Execute a multi layer Perceptron neural network.</span><span class="x">
</span><span class="c">// [mlp:MultiLayerNetwork] multilayer perceptron network pointer, [s:Pattern] input value</span><span class="x">
</span><span class="c">// It returns output values by network</span><span class="x">
</span><span class="k">func</span><span class="x"> </span><span class="n">Execute</span><span class="p">(</span><span class="n">mlp</span><span class="x"> </span><span class="o">*</span><span class="n">MultiLayerNetwork</span><span class="p">,</span><span class="x"> </span><span class="n">s</span><span class="x"> </span><span class="o">*</span><span class="n">Pattern</span><span class="p">,</span><span class="x"> </span><span class="n">options</span><span class="x"> </span><span class="o">...</span><span class="kt">int</span><span class="p">)</span><span class="x"> </span><span class="p">(</span><span class="n">r</span><span class="x"> </span><span class="p">[]</span><span class="kt">float64</span><span class="p">)</span><span class="x"> </span><span class="p">{</span><span class="x">

	</span><span class="c">// new value</span><span class="x">
	</span><span class="n">nv</span><span class="x"> </span><span class="o">:=</span><span class="x"> </span><span class="m">0.0</span><span class="x">

	</span><span class="c">// result of execution for each OUTPUT NeuronUnit in OUTPUT NeuralLayer</span><span class="x">
	</span><span class="n">r</span><span class="x"> </span><span class="o">=</span><span class="x"> </span><span class="nb">make</span><span class="p">([]</span><span class="kt">float64</span><span class="p">,</span><span class="x"> </span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">)</span><span class="o">-</span><span class="m">1</span><span class="p">]</span><span class="o">.</span><span class="n">Length</span><span class="p">)</span><span class="x">

	</span><span class="c">// show pattern to network =&gt;</span><span class="x">
	</span><span class="k">for</span><span class="x"> </span><span class="n">i</span><span class="x"> </span><span class="o">:=</span><span class="x"> </span><span class="m">0</span><span class="p">;</span><span class="x"> </span><span class="n">i</span><span class="x"> </span><span class="o">&lt;</span><span class="x"> </span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">Features</span><span class="p">);</span><span class="x"> </span><span class="n">i</span><span class="o">++</span><span class="x"> </span><span class="p">{</span><span class="x">

		</span><span class="c">// setup value of each neurons in first layers to respective features of pattern</span><span class="x">
		</span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="m">0</span><span class="p">]</span><span class="o">.</span><span class="n">NeuronUnits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">Value</span><span class="x"> </span><span class="o">=</span><span class="x"> </span><span class="n">s</span><span class="o">.</span><span class="n">Features</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="x">

	</span><span class="p">}</span><span class="x">

	</span><span class="c">// execute - hiddens + output</span><span class="x">
	</span><span class="c">// for each layers from first hidden to output</span><span class="x">
	</span><span class="k">for</span><span class="x"> </span><span class="n">k</span><span class="x"> </span><span class="o">:=</span><span class="x"> </span><span class="m">1</span><span class="p">;</span><span class="x"> </span><span class="n">k</span><span class="x"> </span><span class="o">&lt;</span><span class="x"> </span><span class="nb">len</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">);</span><span class="x"> </span><span class="n">k</span><span class="o">++</span><span class="x"> </span><span class="p">{</span><span class="x">

		</span><span class="c">// for each neurons in focused level</span><span class="x">
		</span><span class="k">for</span><span class="x"> </span><span class="n">i</span><span class="x"> </span><span class="o">:=</span><span class="x"> </span><span class="m">0</span><span class="p">;</span><span class="x"> </span><span class="n">i</span><span class="x"> </span><span class="o">&lt;</span><span class="x"> </span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">Length</span><span class="p">;</span><span class="x"> </span><span class="n">i</span><span class="o">++</span><span class="x"> </span><span class="p">{</span><span class="x">

			</span><span class="c">// init new value</span><span class="x">
			</span><span class="n">nv</span><span class="x"> </span><span class="o">=</span><span class="x"> </span><span class="m">0.0</span><span class="x">

			</span><span class="c">// for each neurons in previous level (for k = 1, INPUT)</span><span class="x">
			</span><span class="k">for</span><span class="x"> </span><span class="n">j</span><span class="x"> </span><span class="o">:=</span><span class="x"> </span><span class="m">0</span><span class="p">;</span><span class="x"> </span><span class="n">j</span><span class="x"> </span><span class="o">&lt;</span><span class="x"> </span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="n">k</span><span class="x"> </span><span class="o">-</span><span class="x"> </span><span class="m">1</span><span class="p">]</span><span class="o">.</span><span class="n">Length</span><span class="p">;</span><span class="x"> </span><span class="n">j</span><span class="o">++</span><span class="x"> </span><span class="p">{</span><span class="x">

				</span><span class="c">// sum output value of previous neurons multiplied by weight between previous and focused neuron</span><span class="x">
				</span><span class="n">nv</span><span class="x"> </span><span class="o">+=</span><span class="x"> </span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">NeuronUnits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">Weights</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="x"> </span><span class="o">*</span><span class="x"> </span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="n">k</span><span class="x"> </span><span class="o">-</span><span class="x"> </span><span class="m">1</span><span class="p">]</span><span class="o">.</span><span class="n">NeuronUnits</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">Value</span><span class="x">

				</span><span class="n">log</span><span class="o">.</span><span class="n">WithFields</span><span class="p">(</span><span class="n">log</span><span class="o">.</span><span class="n">Fields</span><span class="p">{</span><span class="x">
					</span><span class="s">"level"</span><span class="o">:</span><span class="x">     </span><span class="s">"debug"</span><span class="p">,</span><span class="x">
					</span><span class="s">"msg"</span><span class="o">:</span><span class="x">       </span><span class="s">"multilayer perceptron execution"</span><span class="p">,</span><span class="x">
					</span><span class="s">"len(mlp.NeuralLayers)"</span><span class="o">:</span><span class="x">  </span><span class="nb">len</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">),</span><span class="x">
					</span><span class="s">"layer:  "</span><span class="o">:</span><span class="x"> </span><span class="n">k</span><span class="p">,</span><span class="x">
					</span><span class="s">"neuron: "</span><span class="o">:</span><span class="x"> </span><span class="n">i</span><span class="p">,</span><span class="x">
					</span><span class="s">"previous neuron: "</span><span class="o">:</span><span class="x"> </span><span class="n">j</span><span class="p">,</span><span class="x">
				</span><span class="p">})</span><span class="o">.</span><span class="n">Debug</span><span class="p">(</span><span class="s">"Compute output propagation."</span><span class="p">)</span><span class="x">

			</span><span class="p">}</span><span class="x">

			</span><span class="c">// add neuron bias</span><span class="x">
			</span><span class="n">nv</span><span class="x"> </span><span class="o">+=</span><span class="x"> </span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">NeuronUnits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">Bias</span><span class="x">

			</span><span class="c">// compute activation function to new output value</span><span class="x">
			</span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">NeuronUnits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">Value</span><span class="x"> </span><span class="o">=</span><span class="x"> </span><span class="n">mlp</span><span class="o">.</span><span class="n">T_func</span><span class="p">(</span><span class="n">nv</span><span class="p">)</span><span class="x">

			</span><span class="n">log</span><span class="o">.</span><span class="n">WithFields</span><span class="p">(</span><span class="n">log</span><span class="o">.</span><span class="n">Fields</span><span class="p">{</span><span class="x">
				</span><span class="s">"level"</span><span class="o">:</span><span class="x">     </span><span class="s">"debug"</span><span class="p">,</span><span class="x">
				</span><span class="s">"msg"</span><span class="o">:</span><span class="x">       </span><span class="s">"setup new neuron output value after transfer function application"</span><span class="p">,</span><span class="x">
				</span><span class="s">"len(mlp.NeuralLayers)"</span><span class="o">:</span><span class="x">  </span><span class="nb">len</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">),</span><span class="x">
				</span><span class="s">"layer:  "</span><span class="o">:</span><span class="x"> </span><span class="n">k</span><span class="p">,</span><span class="x">
				</span><span class="s">"neuron: "</span><span class="o">:</span><span class="x"> </span><span class="n">i</span><span class="p">,</span><span class="x">
				</span><span class="s">"outputvalue"</span><span class="x"> </span><span class="o">:</span><span class="x"> </span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">NeuronUnits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">Value</span><span class="p">,</span><span class="x">
			</span><span class="p">})</span><span class="o">.</span><span class="n">Debug</span><span class="p">(</span><span class="s">"Setup new neuron output value after transfer function application."</span><span class="p">)</span><span class="x">

		</span><span class="p">}</span><span class="x">

	</span><span class="p">}</span><span class="x">


	</span><span class="c">// get ouput values</span><span class="x">
	</span><span class="k">for</span><span class="x"> </span><span class="n">i</span><span class="x"> </span><span class="o">:=</span><span class="x"> </span><span class="m">0</span><span class="p">;</span><span class="x"> </span><span class="n">i</span><span class="x"> </span><span class="o">&lt;</span><span class="x"> </span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">)</span><span class="o">-</span><span class="m">1</span><span class="p">]</span><span class="o">.</span><span class="n">Length</span><span class="p">;</span><span class="x"> </span><span class="n">i</span><span class="o">++</span><span class="x"> </span><span class="p">{</span><span class="x">

		</span><span class="c">// simply accumulate values of all neurons in last level</span><span class="x">
		</span><span class="n">r</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="x"> </span><span class="o">=</span><span class="x"> </span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">)</span><span class="o">-</span><span class="m">1</span><span class="p">]</span><span class="o">.</span><span class="n">NeuronUnits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">Value</span><span class="x">

	</span><span class="p">}</span><span class="x">

	</span><span class="k">return</span><span class="x"> </span><span class="n">r</span><span class="x">

</span><span class="p">}</span></code></pre></figure>

<p>Basically, what <code class="highlighter-rouge">Execute</code> function does is computing the result of execution for each <em>output</em> <code class="highlighter-rouge">NeuronUnit</code> in <em>output</em> <code class="highlighter-rouge">NeuralLayer</code>. In order, it first <em>inserts input</em> to <em>input</em> <code class="highlighter-rouge">NeuralLayer</code> of the network, assigning the values of the dimensions (<code class="highlighter-rouge">Features</code> field) of each pattern to values (<code class="highlighter-rouge">Value</code> field) of each <code class="highlighter-rouge">NeuronUnit</code> in input layer (<code class="highlighter-rouge">mlp.NeuralLayers[0]</code>); after that, for each layers from first hidden to output, and for each neurons in the previous level and the current, execution algorithm computes the sum of multiplication between the weight that links two involved neurons and the (output) computed in the step before of the previous neuron - this is the meaning of most internal for. Then, the bias - natural propension to activation - of the neuron is added to the quantity <em>nv</em>, and output value of the current neuron in the current neural layer is <em>updated</em> with the activation function computed passing this quantity <em>nv</em> as parameter. The last for simply accumulate values of all neurons in last level and return the result. To summarize, this algorithm makes the input flow through the network, using weights to modulate the various dimensions that describe it and the activation functions to calculate the response of each neuron. In the end, the values accumulated in the neurons of the last level are returned.</p>

<p>Back to the <code class="highlighter-rouge">BackPropagate</code>, we already said it starts executing the network. The idea is to get the value accumulated in the neurons of the last level, to compute the error accumulated retracing the various steps backwards. With the (<strong>uncorrect</strong>) assumption of a convex function, we can imagine that <em>solving the weight update task backwards, by calculating the derivative of the activation function</em>, is a good way to <em>go down towards the global optimum</em>. In reality, there is no guarantee of not being <em>stuck in a false minimum</em>, and this depends on the characteristics of the function and (most likely) also on the architecture chosen for our ann.</p>

<p align="center"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/1e/Extrema_example.svg/600px-Extrema_example.svg.png" alt="perceptron" style="width: 300px; marker-top: -10px;" /></p>

<p>Weights update:</p>

<h5 id="weight-update---2-of-2">Weight update - 2 of 2</h5>
<p>For each weight in the network, the following steps must be followed:</p>

<ul>
  <li>the weight’s output delta and input activation are multiplied to find the gradient of the weight,</li>
  <li>a percentage (modulated by learning rate) of the weight’s gradient is subtracted from the weight;</li>
</ul>

<p>The learning rate <em>influences</em> the speed and quality of learning. The greater it is, the faster the neuron trains, but the lower it is, the more accurate the training is. The sign of the gradient of a weight indicates whether the error varies directly with, or inversely to, the weight. Therefore, the weight must be updated in the opposite direction - this is the reason of the name <em>gradient descent</em>.</p>

<figure class="highlight"><pre><code class="language-golang" data-lang="golang"><span class="c">// BackPropagation algorithm.</span><span class="x">
</span><span class="c">// [mlp:MultiLayerNetwork] input value		[s:Pattern] input value (scaled between 0 and 1)</span><span class="x">
</span><span class="c">// [o:[]float64] expected output value (scaled between 0 and 1)</span><span class="x">
</span><span class="c">// return [r:float64] delta error between generated output and expected output</span><span class="x">
</span><span class="k">func</span><span class="x"> </span><span class="n">BackPropagate</span><span class="p">(</span><span class="n">mlp</span><span class="x"> </span><span class="o">*</span><span class="n">MultiLayerNetwork</span><span class="p">,</span><span class="x"> </span><span class="n">s</span><span class="x"> </span><span class="o">*</span><span class="n">Pattern</span><span class="p">,</span><span class="x"> </span><span class="n">o</span><span class="x"> </span><span class="p">[]</span><span class="kt">float64</span><span class="p">,</span><span class="x"> </span><span class="n">options</span><span class="x"> </span><span class="o">...</span><span class="kt">int</span><span class="p">)</span><span class="x"> </span><span class="p">(</span><span class="n">r</span><span class="x"> </span><span class="kt">float64</span><span class="p">)</span><span class="x"> </span><span class="p">{</span><span class="x">

	</span><span class="k">var</span><span class="x"> </span><span class="n">no</span><span class="x"> </span><span class="p">[]</span><span class="kt">float64</span><span class="p">;</span><span class="x">
	</span><span class="c">// execute network with pattern passed over each level to output</span><span class="x">
	</span><span class="k">if</span><span class="x"> </span><span class="nb">len</span><span class="p">(</span><span class="n">options</span><span class="p">)</span><span class="x"> </span><span class="o">==</span><span class="x"> </span><span class="m">1</span><span class="x"> </span><span class="p">{</span><span class="x">
		</span><span class="n">no</span><span class="x"> </span><span class="o">=</span><span class="x"> </span><span class="n">Execute</span><span class="p">(</span><span class="n">mlp</span><span class="p">,</span><span class="x"> </span><span class="n">s</span><span class="p">,</span><span class="x"> </span><span class="n">options</span><span class="p">[</span><span class="m">0</span><span class="p">])</span><span class="x">
	</span><span class="p">}</span><span class="x"> </span><span class="k">else</span><span class="x"> </span><span class="p">{</span><span class="x">
		</span><span class="n">no</span><span class="x"> </span><span class="o">=</span><span class="x"> </span><span class="n">Execute</span><span class="p">(</span><span class="n">mlp</span><span class="p">,</span><span class="x"> </span><span class="n">s</span><span class="p">)</span><span class="x">
	</span><span class="p">}</span><span class="x">

	</span><span class="c">// init error</span><span class="x">
	</span><span class="n">e</span><span class="x"> </span><span class="o">:=</span><span class="x"> </span><span class="m">0.0</span><span class="x">

	</span><span class="c">// compute output error and delta in output layer</span><span class="x">
	</span><span class="k">for</span><span class="x"> </span><span class="n">i</span><span class="x"> </span><span class="o">:=</span><span class="x"> </span><span class="m">0</span><span class="p">;</span><span class="x"> </span><span class="n">i</span><span class="x"> </span><span class="o">&lt;</span><span class="x"> </span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">)</span><span class="o">-</span><span class="m">1</span><span class="p">]</span><span class="o">.</span><span class="n">Length</span><span class="p">;</span><span class="x"> </span><span class="n">i</span><span class="o">++</span><span class="x"> </span><span class="p">{</span><span class="x">

		</span><span class="c">// compute error in output: output for given pattern - output computed by network</span><span class="x">
		</span><span class="n">e</span><span class="x"> </span><span class="o">=</span><span class="x"> </span><span class="n">o</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="x"> </span><span class="o">-</span><span class="x"> </span><span class="n">no</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="x">

		</span><span class="c">// compute delta for each neuron in output layer as:</span><span class="x">
		</span><span class="c">// error in output * derivative of transfer function of network output</span><span class="x">
		</span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">)</span><span class="o">-</span><span class="m">1</span><span class="p">]</span><span class="o">.</span><span class="n">NeuronUnits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">Delta</span><span class="x"> </span><span class="o">=</span><span class="x"> </span><span class="n">e</span><span class="x"> </span><span class="o">*</span><span class="x"> </span><span class="n">mlp</span><span class="o">.</span><span class="n">T_func_d</span><span class="p">(</span><span class="n">no</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="x">

	</span><span class="p">}</span><span class="x">

	</span><span class="c">// backpropagate error to previous layers</span><span class="x">
	</span><span class="c">// for each layers starting from the last hidden (len(mlp.NeuralLayers)-2)</span><span class="x">
	</span><span class="k">for</span><span class="x"> </span><span class="n">k</span><span class="x"> </span><span class="o">:=</span><span class="x"> </span><span class="nb">len</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">)</span><span class="o">-</span><span class="m">2</span><span class="p">;</span><span class="x"> </span><span class="n">k</span><span class="x"> </span><span class="o">&gt;=</span><span class="x"> </span><span class="m">0</span><span class="p">;</span><span class="x"> </span><span class="n">k</span><span class="o">--</span><span class="x"> </span><span class="p">{</span><span class="x">

		</span><span class="c">// compute actual layer errors and re-compute delta</span><span class="x">
		</span><span class="k">for</span><span class="x"> </span><span class="n">i</span><span class="x"> </span><span class="o">:=</span><span class="x"> </span><span class="m">0</span><span class="p">;</span><span class="x"> </span><span class="n">i</span><span class="x"> </span><span class="o">&lt;</span><span class="x"> </span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">Length</span><span class="p">;</span><span class="x"> </span><span class="n">i</span><span class="o">++</span><span class="x"> </span><span class="p">{</span><span class="x">

			</span><span class="c">// reset error accumulator</span><span class="x">
			</span><span class="n">e</span><span class="x"> </span><span class="o">=</span><span class="x"> </span><span class="m">0.0</span><span class="x">

			</span><span class="c">// for each link to next layer</span><span class="x">
			</span><span class="k">for</span><span class="x"> </span><span class="n">j</span><span class="x"> </span><span class="o">:=</span><span class="x"> </span><span class="m">0</span><span class="p">;</span><span class="x"> </span><span class="n">j</span><span class="x"> </span><span class="o">&lt;</span><span class="x"> </span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="n">k</span><span class="x"> </span><span class="o">+</span><span class="x"> </span><span class="m">1</span><span class="p">]</span><span class="o">.</span><span class="n">Length</span><span class="p">;</span><span class="x"> </span><span class="n">j</span><span class="o">++</span><span class="x"> </span><span class="p">{</span><span class="x">

				</span><span class="c">// sum delta value of next neurons multiplied by weight between focused neuron and all neurons in next level</span><span class="x">
				</span><span class="n">e</span><span class="x"> </span><span class="o">+=</span><span class="x"> </span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="n">k</span><span class="x"> </span><span class="o">+</span><span class="x"> </span><span class="m">1</span><span class="p">]</span><span class="o">.</span><span class="n">NeuronUnits</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">Delta</span><span class="x"> </span><span class="o">*</span><span class="x"> </span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="n">k</span><span class="x"> </span><span class="o">+</span><span class="x"> </span><span class="m">1</span><span class="p">]</span><span class="o">.</span><span class="n">NeuronUnits</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">Weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="x">

			</span><span class="p">}</span><span class="x">

			</span><span class="c">// compute delta for each neuron in focused layer as error * derivative of transfer function</span><span class="x">
			</span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">NeuronUnits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">Delta</span><span class="x"> </span><span class="o">=</span><span class="x"> </span><span class="n">e</span><span class="x"> </span><span class="o">*</span><span class="x"> </span><span class="n">mlp</span><span class="o">.</span><span class="n">T_func_d</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">NeuronUnits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">Value</span><span class="p">)</span><span class="x">

		</span><span class="p">}</span><span class="x">

		</span><span class="c">// compute weights in the next layer</span><span class="x">
		</span><span class="c">// for each link to next layer</span><span class="x">
		</span><span class="k">for</span><span class="x"> </span><span class="n">i</span><span class="x"> </span><span class="o">:=</span><span class="x"> </span><span class="m">0</span><span class="p">;</span><span class="x"> </span><span class="n">i</span><span class="x"> </span><span class="o">&lt;</span><span class="x"> </span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="n">k</span><span class="x"> </span><span class="o">+</span><span class="x"> </span><span class="m">1</span><span class="p">]</span><span class="o">.</span><span class="n">Length</span><span class="p">;</span><span class="x"> </span><span class="n">i</span><span class="o">++</span><span class="x"> </span><span class="p">{</span><span class="x">

			</span><span class="c">// for each neurons in actual level (for k = 0, INPUT)</span><span class="x">
			</span><span class="k">for</span><span class="x"> </span><span class="n">j</span><span class="x"> </span><span class="o">:=</span><span class="x"> </span><span class="m">0</span><span class="p">;</span><span class="x"> </span><span class="n">j</span><span class="x"> </span><span class="o">&lt;</span><span class="x"> </span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">Length</span><span class="p">;</span><span class="x"> </span><span class="n">j</span><span class="o">++</span><span class="x"> </span><span class="p">{</span><span class="x">

				</span><span class="c">// sum learning rate * next level next neuron Delta * actual level actual neuron output value</span><span class="x">
				</span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="n">k</span><span class="x"> </span><span class="o">+</span><span class="x"> </span><span class="m">1</span><span class="p">]</span><span class="o">.</span><span class="n">NeuronUnits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">Weights</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="x"> </span><span class="o">+=</span><span class="x">
					</span><span class="n">mlp</span><span class="o">.</span><span class="n">L_rate</span><span class="x"> </span><span class="o">*</span><span class="x"> </span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="n">k</span><span class="x"> </span><span class="o">+</span><span class="x"> </span><span class="m">1</span><span class="p">]</span><span class="o">.</span><span class="n">NeuronUnits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">Delta</span><span class="x"> </span><span class="o">*</span><span class="x"> </span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">NeuronUnits</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">Value</span><span class="x">

			</span><span class="p">}</span><span class="x">

			</span><span class="c">// learning rate * next level next neuron Delta * actual level actual neuron output value</span><span class="x">
			</span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="n">k</span><span class="x"> </span><span class="o">+</span><span class="x"> </span><span class="m">1</span><span class="p">]</span><span class="o">.</span><span class="n">NeuronUnits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">Bias</span><span class="x"> </span><span class="o">+=</span><span class="x"> </span><span class="n">mlp</span><span class="o">.</span><span class="n">L_rate</span><span class="x"> </span><span class="o">*</span><span class="x"> </span><span class="n">mlp</span><span class="o">.</span><span class="n">NeuralLayers</span><span class="p">[</span><span class="n">k</span><span class="x"> </span><span class="o">+</span><span class="x"> </span><span class="m">1</span><span class="p">]</span><span class="o">.</span><span class="n">NeuronUnits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">Delta</span><span class="x">

		</span><span class="p">}</span><span class="x">

	</span><span class="p">}</span><span class="x">

	</span><span class="c">// compute global errors as sum of abs difference between output execution for each neuron in output layer</span><span class="x">
	</span><span class="c">// and desired value in each neuron in output layer</span><span class="x">
	</span><span class="k">for</span><span class="x"> </span><span class="n">i</span><span class="x"> </span><span class="o">:=</span><span class="x"> </span><span class="m">0</span><span class="p">;</span><span class="x"> </span><span class="n">i</span><span class="x"> </span><span class="o">&lt;</span><span class="x"> </span><span class="nb">len</span><span class="p">(</span><span class="n">o</span><span class="p">);</span><span class="x"> </span><span class="n">i</span><span class="o">++</span><span class="x"> </span><span class="p">{</span><span class="x">

		</span><span class="n">r</span><span class="x"> </span><span class="o">+=</span><span class="x"> </span><span class="n">math</span><span class="o">.</span><span class="n">Abs</span><span class="p">(</span><span class="n">no</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="x"> </span><span class="o">-</span><span class="x"> </span><span class="n">o</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="x">

	</span><span class="p">}</span><span class="x">

	</span><span class="c">// average error</span><span class="x">
	</span><span class="n">r</span><span class="x"> </span><span class="o">=</span><span class="x"> </span><span class="n">r</span><span class="x"> </span><span class="o">/</span><span class="x"> </span><span class="kt">float64</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">o</span><span class="p">))</span><span class="x">

	</span><span class="k">return</span><span class="x">

</span><span class="p">}</span></code></pre></figure>

<p>After execution step, <code class="highlighter-rouge">BackPropagate</code> function starts computing output error and delta for the output level. The delta for a given neuron can be calculated as follows:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>delta = (expected - output) * transfer\_derivative(output)
</code></pre></div></div>

<p>where expected is the expected output value (<em>o[i]</em>) for the neuron and output is the output value for the neuron (<em>no[i]</em>) computed by the Execution step (the first operation is done in the code by <em>e = o[i] - no[i]</em> operation). Then, the <em>transfer_derivative()</em> calculates the slope of the neuron’s output value and the algorithm save this value to the delta fields of each of the neurons (not only in the oupput layers): this is done because the layers of the network are iterated in reverse order - or <em>backwards</em>, as it is shown by the <em>k-for</em> (<em>k–</em>) - starting at the output and working backwards. This ensures that the neurons in the output layer have errors values calculated first that neurons in the hidden layer can use in the subsequent iteration.</p>

<p>In the hidden layer, things are a little more complicated. The error signal for a neuron in the hidden layer is computed as the <em>weighted error of each neuron in the output layer</em>. Think of the error traveling back along the weights of the output layer to the neurons in the hidden layer: the back-propagated error signal <strong>is accumulated</strong> and then <strong>used to determine the error for the neuron in the hidden layer</strong>, as follows:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>delta\_i = accumulated(weight\_i * delta\_j) * transfer\_derivative(output)
</code></pre></div></div>

<p>where <em>delta_j</em> is the error signal from the <em>j_th</em> neuron in the output layer, <em>weight_i</em> is the weight that connects the <em>i_th</em> neuron of the output layer to the current neuron, and output is the output of the current neuron<sup id="fnref:details"><a href="#fn:details" class="footnote">3</a></sup>. After that there is the network layers weights update, that follow this rules</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>weight\_i = weight\_i + (learning_rate * delta\_j * input)
</code></pre></div></div>

<p>Finally, the errors (as the abs difference between expcted minus computed) accumulated in the neurons of the last level are returned. Wait a minute: where is the training algorithm?</p>

<h4 id="training-algorithm">Training Algorithm</h4>
<p>Look at the code below! Basically, what it does is running for a fixed amount of epochs the BackPropagate function.</p>

<figure class="highlight"><pre><code class="language-golang" data-lang="golang"><span class="c">// MLPTrain train a mlp MultiLayerNetwork with BackPropagation algorithm for assisted learning.</span><span class="x">
</span><span class="k">func</span><span class="x"> </span><span class="n">MLPTrain</span><span class="p">(</span><span class="n">mlp</span><span class="x"> </span><span class="o">*</span><span class="n">MultiLayerNetwork</span><span class="p">,</span><span class="x"> </span><span class="n">patterns</span><span class="x"> </span><span class="p">[]</span><span class="n">Pattern</span><span class="p">,</span><span class="x"> </span><span class="n">mapped</span><span class="x"> </span><span class="p">[]</span><span class="kt">string</span><span class="p">,</span><span class="x"> </span><span class="n">epochs</span><span class="x"> </span><span class="kt">int</span><span class="p">)</span><span class="x"> </span><span class="p">{</span><span class="x">

	</span><span class="n">epoch</span><span class="x"> </span><span class="o">:=</span><span class="x"> </span><span class="m">0</span><span class="x">
	</span><span class="n">output</span><span class="x"> </span><span class="o">:=</span><span class="x"> </span><span class="nb">make</span><span class="p">([]</span><span class="kt">float64</span><span class="p">,</span><span class="x"> </span><span class="nb">len</span><span class="p">(</span><span class="n">mapped</span><span class="p">))</span><span class="x">

	</span><span class="c">// for fixed number of epochs</span><span class="x">
	</span><span class="k">for</span><span class="x"> </span><span class="p">{</span><span class="x">

		</span><span class="c">// for each pattern in training set</span><span class="x">
		</span><span class="k">for</span><span class="x"> </span><span class="n">_</span><span class="p">,</span><span class="x"> </span><span class="n">pattern</span><span class="x"> </span><span class="o">:=</span><span class="x"> </span><span class="k">range</span><span class="x"> </span><span class="n">patterns</span><span class="x"> </span><span class="p">{</span><span class="x">

			</span><span class="c">// setup desired output for each unit</span><span class="x">
			</span><span class="k">for</span><span class="x"> </span><span class="n">io</span><span class="p">,</span><span class="x"> </span><span class="n">_</span><span class="x"> </span><span class="o">:=</span><span class="x"> </span><span class="k">range</span><span class="x"> </span><span class="n">output</span><span class="x"> </span><span class="p">{</span><span class="x">
				</span><span class="n">output</span><span class="p">[</span><span class="n">io</span><span class="p">]</span><span class="x"> </span><span class="o">=</span><span class="x"> </span><span class="m">0.0</span><span class="x">
			</span><span class="p">}</span><span class="x">
			</span><span class="c">// setup desired output for specific class of pattern focused</span><span class="x">
			</span><span class="n">output</span><span class="p">[</span><span class="kt">int</span><span class="p">(</span><span class="n">pattern</span><span class="o">.</span><span class="n">SingleExpectation</span><span class="p">)]</span><span class="x"> </span><span class="o">=</span><span class="x"> </span><span class="m">1.0</span><span class="x">
			</span><span class="c">// back propagation</span><span class="x">
			</span><span class="n">BackPropagate</span><span class="p">(</span><span class="n">mlp</span><span class="p">,</span><span class="x"> </span><span class="o">&amp;</span><span class="n">pattern</span><span class="p">,</span><span class="x"> </span><span class="n">output</span><span class="p">)</span><span class="x">

		</span><span class="p">}</span><span class="x">

		</span><span class="n">log</span><span class="o">.</span><span class="n">WithFields</span><span class="p">(</span><span class="n">log</span><span class="o">.</span><span class="n">Fields</span><span class="p">{</span><span class="x">
			</span><span class="s">"level"</span><span class="o">:</span><span class="x">             </span><span class="s">"info"</span><span class="p">,</span><span class="x">
			</span><span class="s">"place"</span><span class="o">:</span><span class="x">             </span><span class="s">"validation"</span><span class="p">,</span><span class="x">
			</span><span class="s">"method"</span><span class="o">:</span><span class="x">            </span><span class="s">"MLPTrain"</span><span class="p">,</span><span class="x">
			</span><span class="s">"epoch"</span><span class="o">:</span><span class="x">        	 </span><span class="n">epoch</span><span class="p">,</span><span class="x">
		</span><span class="p">})</span><span class="o">.</span><span class="n">Debug</span><span class="p">(</span><span class="s">"Training epoch completed."</span><span class="p">)</span><span class="x">

		</span><span class="c">// if max number of epochs is reached</span><span class="x">
		</span><span class="k">if</span><span class="x"> </span><span class="n">epoch</span><span class="x"> </span><span class="o">&gt;</span><span class="x"> </span><span class="n">epochs</span><span class="x"> </span><span class="p">{</span><span class="x">
			</span><span class="c">// exit</span><span class="x">
			</span><span class="k">break</span><span class="x">
		</span><span class="p">}</span><span class="x">
		</span><span class="c">// increase number of epoch</span><span class="x">
		</span><span class="n">epoch</span><span class="o">++</span><span class="x">

	</span><span class="p">}</span><span class="x">

</span><span class="p">}</span></code></pre></figure>

<p>Thank you everybody for reading!</p>

<div class="footnotes">
  <ol>
    <li id="fn:rosen">
      <p>F. Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, pages 65–386, 1958. (cit. a p. 5).&nbsp;<a href="#fnref:rosen" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:linsep">
      <p>This condition describes the situation in which there exists a hyperplane able to separate, in the vector space of the inputs, those that require a positive output from those requiring a negative output.&nbsp;<a href="#fnref:linsep" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:details">
      <p>I do not want to bore you with maths, if you want to read more maths details <a href="https://en.wikipedia.org/wiki/Backpropagation">here</a>.&nbsp;<a href="#fnref:details" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>


<div class="comments">
   <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'made2591';

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    

</div>

<hr/>


  
    <div class="post-navs row">
      
        <div class="col-md-6 post-nav">

          <h3 class="section-header">
            Older
            <span class="text-muted"> &middot; </span>
            <a href="/archive">View Archive (35)</a>
          </h3>

          <h2 class="post-title-link"><a href="/posts/fundamentals">Fundamentals by an ITalian guy</a></h2>
          <h3 id="prelude">Prelude</h3>
<p>Nobody ever tells you enough: you need to know the <em>fundamentals</em>. Ok - what am I talking about? I would rather limit myself to talking about computer science, something I really do not know about.</p>


        </div>
      
      

        <div class="col-md-6 post-nav">
          <h3 class="section-header">
            Newer
            
          </h3>

          <h2 class="post-title-link"><a href="/posts/predix-angular-iot">Predix Machine and how to configure them</a></h2>
          <h3 id="predix-machine-and-how-to-configure-them">Predix Machine and how to configure them</h3>
<p>In September, waiting for a contract proposal from the company where I currently work, I wrote a tool for the (more or less) <em>automatic</em> configuration of Predix© Machine. Predix©<sup id="fnref:site"><a href="#fn:site" class="footnote">1</a></sup> is the platform created by <em>GE</em> for Industry 4.0, powered by CloudFoundry<sup id="fnref:pcf"><a href="#fn:pcf" class="footnote">2</a></sup>, to securely connect machines, data, and analytics to improve operational efficiency, help you develop, deploy, and operate industrial apps at the edge and in the cloud. As part of my internship I worked with the predix machine and I shared my work to the GE platform. GE mantains an open <a href="https://github.com/PredixDev">repository</a> with predix tool</p>

<div class="footnotes">
  <ol>
    <li id="fn:site">
      <p>more at <a href="http://predix.io">http://predix.io</a>&nbsp;<a href="#fnref:site" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:pcf">
      <p>more at <a href="http://pivotal.io">http://pivotal.io</a>&nbsp;<a href="#fnref:pcf" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

        </div>
      
    </div>
  



          <div id="footer">
<hr/>
  <div class="pull-left">
    &copy;2019.
    Built with <a href="http://jekyllrb.com/">Jekyll</a> and
    <a href="https://github.com/kirqe/autm-rb">Autm-rb</a>. Thanks to <a href="https://github.com/kirqe">kirqe</a>.
  </div>

  <div class="pull-right">
    <span class="credits pull-right social">
      
        
          <a href="https://github.com/made2591/" target="_blank"><i class="fa fa-github"></i></a>
        
      
        
          <a href="https://twitter.com/made2591" target="_blank"><i class="fa fa-twitter"></i></a>
        
      
        
          <a href="https://linkedin.com/in/mmadeddu" target="_blank"><i class="fa fa-linkedin"></i></a>
        
      
        
          <a href="https://www.facebook.com/matteo.madeddu" target="_blank"><i class="fa fa-facebook"></i></a>
        
      
    </span>
  </div>
</div>

        </section>
    </div>
    </div>
    <script src="/public/javascripts/jquery.min.js"></script>
    <script src="/public/javascripts/bootstrap.min.js"></script>
    <!-- Place your <script> tags here. -->

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script id="dsq-count-scr" src="//made2591-github-io.disqus.com/count.js" async></script>

<!-- disqus comment counter -->
<script type="text/javascript">
	/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'made2591-github-io';

	/* * * DON'T EDIT BELOW THIS LINE * * */
	(function () {
	    var s = document.createElement('script'); s.async = true;
	    s.type = 'text/javascript';
	    s.src = '//' + disqus_shortname + '.disqus.com/count.js';
	    (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
	}());
</script>
<!-- /disqus comment counter -->

<!-- google analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-111283556-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-111283556-1');
</script>
<!-- /google analytics -->

  </body>
</html>
